
@BOOK{Friedman2001-vq,
  title     = "The elements of statistical learning",
  author    = "Friedman, Jerome and Hastie, Trevor and Tibshirani, Robert",
  publisher = "Springer series in statistics New York",
  volume    =  1,
  year      =  2001
}

@BOOK{Hastie2015-gy,
  title     = "Statistical Learning with Sparsity: The Lasso and
               Generalizations",
  author    = "Hastie, Trevor and Tibshirani, Robert and Wainwright, Martin",
  abstract  = "Discover New Methods for Dealing with High-Dimensional Data A
               sparse statistical model has only a small number of nonzero
               parameters or weights; therefore, it is much easier to estimate
               and interpret than a dense model. Statistical Learning with
               Sparsity: The Lasso and Generalizations presents methods that
               exploit sparsity to help recover the underlying signal in a set
               of data. Top experts in this rapidly evolving field, the authors
               describe the lasso for linear regression and a simple coordinate
               descent algorithm for its computation. They discuss the
               application of l1 penalties to generalized linear models and
               support vector machines, cover generalized penalties such as the
               elastic net and group lasso, and review numerical methods for
               optimization. They also present statistical inference methods
               for fitted (lasso) models, including the bootstrap, Bayesian
               methods, and recently developed approaches. In addition, the
               book examines matrix decomposition, sparse multivariate
               analysis, graphical models, and compressed sensing. It concludes
               with a survey of theoretical results for the lasso. In this age
               of big data, the number of features measured on a person or
               object can be large and might be larger than the number of
               observations. This book shows how the sparsity assumption allows
               us to tackle these problems and extract useful and reproducible
               patterns from big datasets. Data analysts, computer scientists,
               and theorists will appreciate this thorough and up-to-date
               treatment of sparse statistical modeling.",
  publisher = "Taylor \& Francis",
  month     =  "7~" # may,
  year      =  2015,
  language  = "en"
}

@BOOK{James2014-xw,
  title     = "An Introduction to Statistical Learning: with Applications in
               {R}",
  author    = "James, Gareth and Witten, Daniela and Hastie, Trevor and
               Tibshirani, Robert",
  abstract  = "An Introduction to Statistical Learning provides an accessible
               overview of the field of statistical learning, an essential
               toolset for making sense of the vast and complex data sets that
               have emerged in fields ranging from biology to finance to
               marketing to astrophysics in the past twenty years. This book
               presents some of the most important modeling and prediction
               techniques, along with relevant applications. Topics include
               linear regression, classification, resampling methods, shrinkage
               approaches, tree-based methods, support vector machines,
               clustering, and more. Color graphics and real-world examples are
               used to illustrate the methods presented. Since the goal of this
               textbook is to facilitate the use of these statistical learning
               techniques by practitioners in science, industry, and other
               fields, each chapter contains a tutorial on implementing the
               analyses and methods presented in R, an extremely popular open
               source statistical software platform.Two of the authors co-wrote
               The Elements of Statistical Learning (Hastie, Tibshirani and
               Friedman, 2nd edition 2009), a popular reference book for
               statistics and machine learning researchers. An Introduction to
               Statistical Learning covers many of the same topics, but at a
               level accessible to a much broader audience. This book is
               targeted at statisticians and non-statisticians alike who wish
               to use cutting-edge statistical learning techniques to analyze
               their data. The text assumes only a previous course in linear
               regression and no knowledge of matrix algebra.",
  publisher = "Springer New York",
  month     =  "11~" # jul,
  year      =  2014,
  language  = "en"
}


@ARTICLE{Tibshirani1996-fg,
  title     = "Regression Shrinkage and Selection via the Lasso",
  author    = "Tibshirani, Robert",
  abstract  = "We propose a new method for estimation in linear models. The
               `lasso' minimizes the residual sum of squares subject to the sum
               of the absolute value of the coefficients being less than a
               constant. Because of the nature of this constraint it tends to
               produce some coefficients that are exactly 0 and hence gives
               interpretable models. Our simulation studies suggest that the
               lasso enjoys some of the favourable properties of both subset
               selection and ridge regression. It produces interpretable models
               like subset selection and exhibits the stability of ridge
               regression. There is also an interesting relationship with
               recent work in adaptive function estimation by Donoho and
               Johnstone. The lasso idea is quite general and can be applied in
               a variety of statistical models: extensions to generalized
               regression models and tree-based models are briefly described.",
  journal   = "Journal of the Royal Statistical Society. Series B, Statistical
               methodology",
  publisher = "[Royal Statistical Society, Wiley]",
  volume    =  58,
  number    =  1,
  pages     = "267-288",
  year      =  1996,
  url       = "http://www.jstor.org/stable/2346178"
}

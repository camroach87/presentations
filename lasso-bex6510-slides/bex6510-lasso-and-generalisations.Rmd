---
title: 'BEX6510: Lasso and generalisations'
author: "Cameron Roach"
date: '`r format(Sys.time(), "%d %B, %Y")`'
output:
  ioslides_presentation:
    smaller: yes
    widescreen: yes
  beamer_presentation:
    toc: yes
  slidy_presentation: default
nocite: |
  @Friedman2001-vq, @Hastie2015-gy, @James2014-xw
bibliography: library.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = TRUE,
	message = FALSE,
	warning = FALSE
)

set.seed(1234)
```

# The lasso

## Introduction

* Introduced by @Tibshirani1996-fg (21,216 citations!).
* Allows for automatic variable selection.
* Optimisation problem is convex and can be solved efficiently.


## Motivation

We are given $N$ samples $\left\{ (\mathbf{x}_i, y_i) \right\}_{i=1}^N$ where $\mathbf{x}_i = (x_{i1}, x_{i2}, \ldots, x_{ip})$ and all $x_{ij}, y_i \in \mathbb{R}$. We wish to approximate the response $y_i$. This can be done using a standard OLS approach, but there are drawbacks:

* _Prediction accuracy_: low bias but large variance. Bias-variance trade off can improve prediction accuracy.
* _Interpretation_: difficult if $p$ is large.

The lasso addresses these two issues.

## Ridge regression

Ridge regression predates the lasso. Combines the the least-squares loss with an $\ell_2$ penalty. It finds the solution to

$$
\min_{\beta_0, \boldsymbol{\beta}}  \left\{ 
  \frac{1}{2N} \sum^N_{i=1} \left(y_i -\beta_0 - \sum^p_{j=1} \beta_j x_{ij} \right)^2 +
  \lambda \sum^p_{j=1} \beta_j^2
\right\},
$$

where $\lambda \geq 0$ is the regularization weight.

This can also be expressed in matrix form

$$
\min_{\boldsymbol{\beta}}  \left\{
  \frac{1}{2N} \| \mathbf{y} - \mathbf{X}\boldsymbol{\beta} \|^2_2 + \lambda \| \boldsymbol{\beta} \|_2
\right\}.
$$

Differentiating with respect to $\boldsymbol{\beta}$ and setting equal to zero yields the solution

$$
\hat{\boldsymbol{\beta}} = (\mathbf{X}'\mathbf{X} + \lambda \mathbf{I})^{-1} \mathbf{X}'\mathbf{y}.
$$



## The lasso

The lasso is similar to ridge regression, but uses an $\ell_1$ penalty instead of $\ell_2$. It finds the solution to

$$
\min_{\beta_0, \boldsymbol{\beta}}  \left\{ 
  \frac{1}{2N} \sum^N_{i=1} \left(y_i -\beta_0 - \sum^p_{j=1} \beta_j x_{ij} \right)^2 +
  \lambda \sum^p_{j=1} \left| \beta_j \right|
\right\},
$$

where $\lambda \geq 0$ is the regularization weight.

We can remove the intercept term $\beta_0$ by scaling the data.

This can also be expressed in matrix form

$$
\min_{\boldsymbol{\beta}}  \left\{ 
  \frac{1}{2N} \| \mathbf{y} - \mathbf{X}\boldsymbol{\beta} \|^2_2 + \lambda \| \boldsymbol{\beta} \|_1
\right\}.
$$

Lasso is a convex quadratic programming problem.

The factor $\frac{1}{2N}$ can be omitted, but does make $\lambda$ comparable for different sample sizes [@Hastie2015-gy].

<!-- ---- -->

<!-- Necessary and sufficient condition for solution takes the form -->

<!-- $$ -->
<!-- -\frac{1}{N}\left< \mathbf{x}_j, \mathbf{y} - \mathbf{X} \boldsymbol{\beta}\right> + \lambda s_j = 0, \quad j=1,\ldots,p, -->
<!-- $$ -->

<!-- where $s_j$ -->

## Properties

The estimates of $\boldsymbol{\beta}$ given by ridge regression and the lasso have different properties despite their similarity.

* Ridge regression pushes coefficients towards zero, whereas lasso can make certain coefficients exactly equal to zero.
* Lasso handles sparsity well (only a few non-zero columns).
* Lasso does not handle highly correlated predictors well.

----

### Parameter estimation

<div class="centered">
![Lasso (left) and ridge regression (right) parameter estimation. Solid blue represents constraints regions and red lines show the contours of the RSS function. The OLS solution is given by $\hat{\beta}$. Moving away from the OLS solution results in increasing RSS. The point at which the RSS function contour first touches the constraint region is the lasso/ridge solution. Image: @Friedman2001-vq.](figures/lasso-ridge-estimation.png)
</div>

----

### Coefficient paths

<div class="centered">
![Coefficient paths of the lasso (left) and ridge regression (right). Image: @James2014-xw.](figures/coefficients-path.png)
</div>

----

### Bias-variance tradeoff

<div class="centered">
![Squared bias (black), variance (green) and test MSE (pink) of lasso ona a simulated data set. Image: @James2014-xw.](figures/bias-variance-lasso.png)
</div>

# Computation

## Soft thresholding

<!-- The lasso is a quadratic programming problem with a convex constraint. -->

Given the samples $\left\{ (x_i, y_i \right\}_{i=1}^N$ we wish to solve

$$
\min_{\beta}  \left\{ 
  \frac{1}{2N} \sum^N_{i=1} \left(y_i - \beta x_i \right)^2 + \left| \beta \right|
\right\}.
$$

Here we assume $\mathbf{x}$ and $\mathbf{y}$ have been standardised so that $\frac{1}{N} \sum_{i=1}^N y_i = 0$, $\frac{1}{N} \sum_{i=1}^N x_i = 0$ and $\frac{1}{N} \sum_{i=1}^N x_i^2 = 1$.

Normally in a univariate setting we would simply take the gradient, set it to zero and solve for $\beta$. However, $\left| \beta \right|$ does not have a gradient at $\beta = 0$. Instead, we can see by inspection that

$$
\hat{\beta} = \begin{cases}
  \frac{1}{N} \left< \mathbf{x}, \mathbf{y}\right> - \lambda, & \text{if} &\frac{1}{N} \left< \mathbf{x}, \mathbf{y}\right> > \lambda \\
  0, & \text{if} &\frac{1}{N} |\left< \mathbf{x}, \mathbf{y}\right>| \leq \lambda \\
  \frac{1}{N} \left< \mathbf{x}, \mathbf{y}\right> + \lambda, & \text{if} & \frac{1}{N} \left< \mathbf{x}, \mathbf{y}\right> < -\lambda.
\end{cases}
$$

---

This can be written as

$$
\hat{\beta} = \mathcal{S}_\lambda\left(\frac{1}{N} \left< \mathbf{x}, \mathbf{y}\right> \right),
$$

where 

$$
\mathcal{S}_\lambda\ = \text{sign}(x)(|x| - \lambda)_+
$$

is the _soft-thresholding operator_ and $x_+$ is equal to $x$ if $x>0$ and zero otherwise.

## Cyclic coordinate descent

In the case of multiple predictors we can use _cyclic coordinate descent_. The basic idea is to cycle through predictors in a fixed order and update a single coefficient at each step while holding others constant.

At the $j^{\text{th}}$ step, coefficient $\beta_j$ is updated while $\left\{ \beta_k | k \neq j \right\}$ are held constant. The optimisation problem becomes

$$
\min_{\beta_0, \boldsymbol{\beta}}  \left\{ 
  \frac{1}{2N} \sum^N_{i=1} \left(y_i - \sum_{k\neq j} \beta_k x_{ik} - \beta_j x_{ij} \right)^2 +
  \lambda \sum_{k \neq j} \left| \beta_k \right| + \lambda \left| \beta_j \right|
\right\}.
$$

The solution for $\beta_j$ can be expressed in terms of the partial residual $r_i^{(j)} = y_i - \sum_{k \neq j} \hat{\beta}_k x_{ik}$. We can now use the soft-thresholding operator to update $\beta_j$

$$
\hat{\beta}_j = \mathcal{S}_\lambda\left(\frac{1}{N} \left< \mathbf{x}_j, \mathbf{r}^{(j)} \right> \right).
$$

----

A sufficient condition for coordinate descent to converge to a global minimum is for our convex cost function $f$ to be continuously differentiable and strictly convex. Clearly this is too restrictive when trying to solve for the lasso, but we can get around this with a separability condition

$$
f(\beta_1, \ldots, \beta_p) = g(\beta_1, \ldots, \beta_p) + \sum_{j=1}^p h_j(\beta_j),
$$

where $g: \mathbb{R}^p \rightarrow \mathbb{R}$ is differentiable and convex, and $h: \mathbb{R} \rightarrow \mathbb{R}$ is convex, but not necessarily differentiable. Any convex cost function with this separability condition will converge [@Tseng2001-lk].

In the case of the lasso, we have

$$
\begin{aligned}
  g(\beta) &= \frac{1}{N} \| \mathbf{y} - \mathbf{X}\beta \|_2^2 \\
  h_j(\beta_j) &= \lambda |\beta_j|,
\end{aligned}
$$

and so the coordinate descent algorithm will converge to a global minimum.

There are some generalisations of the lasso (e.g. fused lasso) which violate this separability condition which can result in coordinate descent becoming stuck and failing to reach the global minimum.

# Some theory

## Bounds on lasso $\ell_2$-error

See printout.

<!-- #### Theorem -->

<!-- Suppose that the model matrix $\mathbf{X}$ satisifes the restricted eigenvalue bound for $\gamma > 0$ and constraint set $\mathcal{C}$ -->

<!-- $$ -->
<!-- \frac{\frac{1}{N} \nu \mathbf{X}'\mathbf{X} \nu}{\| \nu \|_2^2} \geq \gamma \text{ for all nonzero} -->
<!-- $$ -->

## Variable selection consistency

See printout.


# Generalisations of the lasso

## Elastic net

The lasso does not handle highly correlated variables well.

* Coefficient paths can be erratic.
* If data is augmented with an identical copy of a feature then coefficients for this feature will not be defined using lasso.
* A quadratic penalty will divide the coefficient equally across these two identical features.

Can combine the ridge and lasso penalties to obtain the _elastic net_, which solves

$$
\min_{\beta_0, \boldsymbol{\beta}}  \left\{ 
  \frac{1}{2} \sum^N_{i=1} \left(y_i -\beta_0 - \boldsymbol{\beta} x_i \right)^2 +
  \lambda \left( \frac{1}{2}(1-\alpha) \left\| \boldsymbol{\beta} \right\|_2^2 +\alpha \left\| \boldsymbol{\beta} \right\|_1 \right)
\right\},
$$

where $\alpha \in [0,1]$ is a parameter that varies the $\ell_1$ or $\ell_2$ penalty and $\lambda \geq 0$ is again a regularization weight. For $\alpha \neq 1$ the elastic net is strictly convex - a unique solution exists.

Again, covariates can be centered to remove the intercept $\beta_0$ which may then be calculated by $\hat{\beta}_0 = \bar{y}$.

----

<div class="centered">
![Comparison of elastic net constraints (left) and lasso (right) in $\mathbb{R}^3$. Curved contours encourage strongly correlated variables to share coefficients. Image: @Hastie2015-gy.](figures/elastic-net-constraint.png)
</div>

## Group lasso

Sometimes covariates have a group structure and it is preferable to have all coefficients within a group shrunk to zero simultaneously.

Let $Z_j \in \mathbb{R}^{p_j}$ be the covariates in group $j \in 1,\ldots,J$. We wish to predict $Y \in \mathbb{R}$ based on the $Z_j$

$$
\mathbb{E}(Y|Z) = \theta_0 + \sum_{j=1}^J Z_j' \theta_j,
$$

where $\theta_j \in \mathbb{R}^{p_j}$ is a group of regression coefficients.

Given $N$ samples $\left\{ y_i, z_{i1}, \ldots, z_{iJ} \right\}_{i=1}^n$ we wish to solve the convex problem

$$
\min_{\theta_0 \in \mathbb{R}, \theta_j \in \mathbb{R}^{p_j}}
\left\{
  \frac{1}{2} \sum_{i=1}^n \left( y_i - \theta_0 - \sum_{j=1}^J z_{ij}'\theta_j \right)^2 + \lambda \sum_{j=1}^J \| \theta_j\|_2
\right\}.
$$

----

Note that

* depending on $\lambda \geq 0$ either the entire vector $\hat{\theta}_j$ will be zero, or all elements will be non-zero.
* if $p_j = 1$ for all $j$ we have $\| \theta_j\|_2 = |\theta_j|$ and so the group lasso reduces to the ordinary lasso.

----
<div class="centered">
![Comparison of group lasso constraints (left) and lasso (right) in $\mathbb{R}^3$. In the group lasso case there are two groups $\theta_1 = (\beta_1, \beta_2)$ and $\theta_2 = \beta_3$. The pointy bit at the top is where $\theta_1$ (and by extension $\beta_1$ and $\beta_2$) will be set to zero. Image: @Hastie2015-gy.](figures/group-lasso-constraint.png)
</div>

----
### Sparsity with group lasso

Sometimes it is necessary to have sparsity with respect to which groups are non-zero and which coefficients within a group are non-zero. Can enforce within-group sparsity by applying an additional $\ell_1$ norm, which gives the convex problem

$$
\min_{\theta_0 \in \mathbb{R}, \theta_j \in \mathbb{R}^{p_j}}
\left\{
  \frac{1}{2} \sum_{i=1}^n \left( y_i - \theta_0 - \sum_{j=1}^J z_{ij}'\theta_j \right)^2 + 
  \lambda \sum_{j=1}^J \left[ (1-\alpha) \|\theta_j\|_2  + \alpha \|\theta_j\|_1 \right]
\right\},
$$

where $\alpha \in [0,1]$.

## Fused lasso

<div class="centered">
![Fused lasso example using comparative genomic hybridization data. The green line is the fused lasso estimate. Image: @Hastie2015-gy.](figures/fused-lasso-example.png)
</div>

----

If we are dealing with data that we expect to be piecewise-constant we may wish to to incorporate smoothing.

The _fused lasso_ exploits this structure. 
Given $N$ samples (and coefficients) we solve the optimization problem

$$
\min_{\theta \in \mathbb{R}^N}
\left\{
  \frac{1}{2} \sum_{i=1}^N (y_i - \theta_i)^2 
  + \lambda_1 \sum_{i=1}^N | \theta_i|
  + \lambda_2 \sum_{i=2}^N | \theta_i - \theta_{i-1}|
\right\}
$$

where $\lambda_1, \lambda_2 \geq 0$ are the regularization weights.

The $\lambda_1$ penalty functions as the $\ell_1$ penalty.

The $\lambda_2$ penalty encourages neighbouring coefficients to be similar.

----

* The piecewise-constant notion can be expanded to handle more general neighbourhoods, such as pixels in an image. Simply adjust the second penalty to sum over all neighbouring pairs $\lambda_2 \sum_{i \sim i'} | \theta_i - \theta_{i'}|$.
* Instead of associating every observation with a coefficient we can solve for $p$ predictors

$$
\min_{\beta_0, \boldsymbol{\beta}}
\left\{
  \frac{1}{2} \sum_{i=1}^N (y_i - \beta_0 - \sum_j^p x_{ij}\beta_j)^2 
  + \lambda_1 \sum_{j=1}^p | \beta_j|
  + \lambda_2 \sum_{j=2}^p | \beta_j - \beta_{j-1}|
\right\}.
$$

# Lasso with R

## A quick example

A quick example using `R`'s `caret` and `glmnet` packages. Can use `glmnet` directly, but `caret` provides extra functionality including

* a consistent interface between many packages,
* cross-validation, and
* grid search for parameter tuning.

```{r caret-glmnet}
require(caret)
require(ISLR)

Hitters <- na.omit(Hitters)

hitters_train <- train(Salary~.,
                       data = Hitters,
                       method = "glmnet",
                       metric = "RMSE",
                       tuneGrid = expand.grid(alpha = 1,
                                              lambda = 10^seq(1,-2,length=25)),
                       trControl = trainControl(method = "cv",
                                                number = 5))
```

----

### Hitters dataset

```{r hitters-data, echo=FALSE}
require(knitr)
require(kableExtra)

kable(Hitters, "html") %>%
  kable_styling() %>%
  scroll_box(width = "1000px", height = "500px")
```

----

```{r cv-plot, fig.align='center'}
require(ggplot2)

ggplot(hitters_train$results, aes(x = lambda, y = RMSE)) + 
  geom_line() +
  labs(title = "RMSE of lasso regression using 5-fold cross validation.")
```

----

```{r glm-best-tune}
hitters_train$bestTune
```

----

```{r glm-coef}
coef(hitters_train$finalModel, s = hitters_train$bestTune$lambda)
```

## References and further reading {-}